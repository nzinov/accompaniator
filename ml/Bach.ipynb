{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import structures\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'structures.Song'>\n"
     ]
    }
   ],
   "source": [
    "Songs = pickle.load(open(\"bach.pickle\", \"rb\"))\n",
    "\n",
    "print(type(Songs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chords = []\n",
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in Songs:\n",
    "    for track in song.tracks:\n",
    "        for chord in track.chords: \n",
    "            new_chord = np.zeros(5-len(chord.notes))\n",
    "            new_chord = np.append(np.array([note.number for note in chord.notes]), new_chord)\n",
    "          #  print(new_chord)\n",
    "            all_chords.append(new_chord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5665\n"
     ]
    }
   ],
   "source": [
    "print(len(all_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "unique_chords = np.unique(all_chords, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 23. 25. 29.  0.]\n",
      " [22. 23. 25. 30.  0.]\n",
      " [22. 23. 26. 31.  0.]\n",
      " ...\n",
      " [29. 31. 33.  0.  0.]\n",
      " [29. 32.  0.  0.  0.]\n",
      " [29. 33.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(unique_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n"
     ]
    }
   ],
   "source": [
    "size = len(unique_chords)\n",
    "unique_chords = list(unique_chords)\n",
    "coded_chords = []\n",
    "\n",
    "\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chord in all_chords:\n",
    "    for i, unique_chord in enumerate(unique_chords):\n",
    "        if np.array_equal(chord, unique_chord):\n",
    "            coded_chords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5665\n"
     ]
    }
   ],
   "source": [
    "print(len(coded_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "remove_indexes = []\n",
    "\n",
    "for i, frec in enumerate(Counter(coded_chords).values()):\n",
    "    if frec == 1:\n",
    "        remove_indexes.append(i)\n",
    "        \n",
    "print(len(remove_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 23. 27. 30.  0.]\n",
      " [22. 23. 27. 32.  0.]\n",
      " [22. 24. 25. 29.  0.]\n",
      " ...\n",
      " [29. 31.  0.  0.  0.]\n",
      " [29. 32.  0.  0.  0.]\n",
      " [29. 33.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "unique_chords = np.delete(unique_chords, remove_indexes, axis=0)\n",
    "#coded_chords = np.delete(coded_chords, remove_indexes)\n",
    "\n",
    "print(unique_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    }
   ],
   "source": [
    "size = len(unique_chords)\n",
    "coded_chords = []\n",
    "\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chord in all_chords:\n",
    "    for i, unique_chord in enumerate(unique_chords):\n",
    "        if np.array_equal(chord, unique_chord):\n",
    "            coded_chords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5586\n"
     ]
    }
   ],
   "source": [
    "print(len(coded_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_chords = all_chords\n",
    "all_chords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chord in old_chords:\n",
    "    for unique_chord in unique_chords:\n",
    "        if np.array_equal(chord, unique_chord):\n",
    "            all_chords.append(chord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5586\n"
     ]
    }
   ],
   "source": [
    "print(len(all_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "798.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5586 / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [coded_chords[i:i + 7] for i in range(0, len(coded_chords)-6, 1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5580, 7)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data = np.reshape(data, (-1, 7))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 6)\n",
      "(5022, 6)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "\n",
    "X = data[:,0:6]\n",
    "y = data[:,6]\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.9, random_state=0)\n",
    "print(test_X.shape)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 31s - loss: 4.7371 - acc: 0.0589 - val_loss: 4.5209 - val_acc: 0.0556\n",
      "1 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 31s - loss: 4.6475 - acc: 0.0573 - val_loss: 4.4851 - val_acc: 0.0556\n",
      "1 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 31s - loss: 4.6392 - acc: 0.0540 - val_loss: 4.4978 - val_acc: 0.0520\n",
      "1 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 31s - loss: 4.6230 - acc: 0.0597 - val_loss: 4.4842 - val_acc: 0.0412\n",
      "1 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 31s - loss: 4.6089 - acc: 0.0573 - val_loss: 4.4878 - val_acc: 0.0645\n",
      "1 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 33s - loss: 4.6166 - acc: 0.0558 - val_loss: 4.5001 - val_acc: 0.0556\n",
      "1 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 36s - loss: 4.6164 - acc: 0.0581 - val_loss: 4.5174 - val_acc: 0.0556\n",
      "1 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.6177 - acc: 0.0577 - val_loss: 4.4897 - val_acc: 0.0556\n",
      "1 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 33s - loss: 4.6157 - acc: 0.0566 - val_loss: 4.4995 - val_acc: 0.0484\n",
      "1 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 33s - loss: 4.7277 - acc: 0.0564 - val_loss: 4.5018 - val_acc: 0.0556\n",
      "2 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.6579 - acc: 0.0526 - val_loss: 4.4740 - val_acc: 0.0556\n",
      "2 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 33s - loss: 4.6234 - acc: 0.0516 - val_loss: 4.4971 - val_acc: 0.0556\n",
      "2 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.6338 - acc: 0.0544 - val_loss: 4.4795 - val_acc: 0.0556\n",
      "2 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.6220 - acc: 0.0579 - val_loss: 4.4948 - val_acc: 0.0556\n",
      "2 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.6135 - acc: 0.0568 - val_loss: 4.4738 - val_acc: 0.0556\n",
      "2 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 4.6166 - acc: 0.0558 - val_loss: 4.4907 - val_acc: 0.0591\n",
      "2 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 4.6209 - acc: 0.0589 - val_loss: 4.4969 - val_acc: 0.0556\n",
      "2 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 4.6151 - acc: 0.0540 - val_loss: 4.4845 - val_acc: 0.0556\n",
      "2 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.7332 - acc: 0.0544 - val_loss: 4.5225 - val_acc: 0.0412\n",
      "3 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.6737 - acc: 0.0550 - val_loss: 4.4778 - val_acc: 0.0556\n",
      "3 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.6369 - acc: 0.0564 - val_loss: 4.5032 - val_acc: 0.0556\n",
      "3 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 36s - loss: 4.6326 - acc: 0.0587 - val_loss: 4.5035 - val_acc: 0.0412\n",
      "3 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 36s - loss: 4.6181 - acc: 0.0556 - val_loss: 4.5018 - val_acc: 0.0412\n",
      "3 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.6098 - acc: 0.0564 - val_loss: 4.4872 - val_acc: 0.0556\n",
      "3 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.6066 - acc: 0.0556 - val_loss: 4.4972 - val_acc: 0.0556\n",
      "3 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.6175 - acc: 0.0562 - val_loss: 4.4782 - val_acc: 0.0556\n",
      "3 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.6054 - acc: 0.0613 - val_loss: 4.4999 - val_acc: 0.0502\n",
      "3 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.7470 - acc: 0.0520 - val_loss: 4.5178 - val_acc: 0.0556\n",
      "4 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.6679 - acc: 0.0575 - val_loss: 4.4981 - val_acc: 0.0556\n",
      "4 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.6284 - acc: 0.0560 - val_loss: 4.4817 - val_acc: 0.0591\n",
      "4 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.6286 - acc: 0.0585 - val_loss: 4.5058 - val_acc: 0.0556\n",
      "4 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.6166 - acc: 0.0571 - val_loss: 4.4961 - val_acc: 0.0502\n",
      "4 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.6296 - acc: 0.0569 - val_loss: 4.5021 - val_acc: 0.0556\n",
      "4 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.6117 - acc: 0.0518 - val_loss: 4.4973 - val_acc: 0.0556\n",
      "4 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.6216 - acc: 0.0613 - val_loss: 4.5084 - val_acc: 0.0556\n",
      "4 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.6212 - acc: 0.0577 - val_loss: 4.5128 - val_acc: 0.0556\n",
      "4 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.7393 - acc: 0.0552 - val_loss: 4.5194 - val_acc: 0.0556\n",
      "5 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.6779 - acc: 0.0585 - val_loss: 4.5181 - val_acc: 0.0412\n",
      "5 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.6367 - acc: 0.0540 - val_loss: 4.5014 - val_acc: 0.0412\n",
      "5 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.6209 - acc: 0.0573 - val_loss: 4.4955 - val_acc: 0.0556\n",
      "5 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.6262 - acc: 0.0512 - val_loss: 4.4755 - val_acc: 0.0573\n",
      "5 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.6205 - acc: 0.0571 - val_loss: 4.4965 - val_acc: 0.0556\n",
      "5 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.6164 - acc: 0.0607 - val_loss: 4.4931 - val_acc: 0.0556\n",
      "5 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.6226 - acc: 0.0581 - val_loss: 4.4785 - val_acc: 0.0556\n",
      "5 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.6163 - acc: 0.0585 - val_loss: 4.4838 - val_acc: 0.0412\n",
      "5 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.7505 - acc: 0.0506 - val_loss: 4.5227 - val_acc: 0.0573\n",
      "6 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 42s - loss: 4.6544 - acc: 0.0566 - val_loss: 4.5055 - val_acc: 0.0573\n",
      "6 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.6393 - acc: 0.0583 - val_loss: 4.4943 - val_acc: 0.0556\n",
      "6 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 42s - loss: 4.6156 - acc: 0.0544 - val_loss: 4.4933 - val_acc: 0.0556\n",
      "6 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 42s - loss: 4.6041 - acc: 0.0587 - val_loss: 4.5035 - val_acc: 0.0556\n",
      "6 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.6156 - acc: 0.0573 - val_loss: 4.4995 - val_acc: 0.0591\n",
      "6 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.5813 - acc: 0.0649 - val_loss: 4.4196 - val_acc: 0.0645\n",
      "6 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.6130 - acc: 0.0591 - val_loss: 4.4932 - val_acc: 0.0681\n",
      "6 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.6152 - acc: 0.0583 - val_loss: 4.4834 - val_acc: 0.0717\n",
      "6 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.7623 - acc: 0.0591 - val_loss: 4.5321 - val_acc: 0.0556\n",
      "7 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.6430 - acc: 0.0575 - val_loss: 4.5031 - val_acc: 0.0412\n",
      "7 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.6327 - acc: 0.0540 - val_loss: 4.4789 - val_acc: 0.0556\n",
      "7 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.6234 - acc: 0.0589 - val_loss: 4.5029 - val_acc: 0.0430\n",
      "7 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.6180 - acc: 0.0597 - val_loss: 4.4797 - val_acc: 0.0412\n",
      "7 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.6245 - acc: 0.0558 - val_loss: 4.5167 - val_acc: 0.0573\n",
      "7 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.6059 - acc: 0.0534 - val_loss: 4.4869 - val_acc: 0.0556\n",
      "7 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 46s - loss: 4.6194 - acc: 0.0554 - val_loss: 4.4810 - val_acc: 0.0556\n",
      "7 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.6017 - acc: 0.0566 - val_loss: 4.4975 - val_acc: 0.0412\n",
      "7 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.7574 - acc: 0.0575 - val_loss: 4.5144 - val_acc: 0.0556\n",
      "8 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.6585 - acc: 0.0560 - val_loss: 4.4979 - val_acc: 0.0556\n",
      "8 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.6270 - acc: 0.0577 - val_loss: 4.4982 - val_acc: 0.0430\n",
      "8 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.6289 - acc: 0.0526 - val_loss: 4.4946 - val_acc: 0.0609\n",
      "8 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.6152 - acc: 0.0536 - val_loss: 4.4760 - val_acc: 0.0591\n",
      "8 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.6085 - acc: 0.0568 - val_loss: 4.4785 - val_acc: 0.0484\n",
      "8 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 48s - loss: 4.6036 - acc: 0.0603 - val_loss: 4.4729 - val_acc: 0.0466\n",
      "8 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.6064 - acc: 0.0581 - val_loss: 4.5377 - val_acc: 0.0609\n",
      "8 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.209294). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 70s - loss: 4.6116 - acc: 0.0571 - val_loss: 4.5005 - val_acc: 0.0520\n",
      "8 9\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 56s - loss: 4.7468 - acc: 0.0510 - val_loss: 4.5319 - val_acc: 0.0412\n",
      "9 1\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.6669 - acc: 0.0528 - val_loss: 4.5055 - val_acc: 0.0502\n",
      "9 2\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.6385 - acc: 0.0534 - val_loss: 4.4884 - val_acc: 0.0556\n",
      "9 3\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.6163 - acc: 0.0542 - val_loss: 4.4865 - val_acc: 0.0556\n",
      "9 4\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.6210 - acc: 0.0568 - val_loss: 4.4968 - val_acc: 0.0556\n",
      "9 5\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.6197 - acc: 0.0552 - val_loss: 4.4896 - val_acc: 0.0556\n",
      "9 6\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.6063 - acc: 0.0552 - val_loss: 4.4659 - val_acc: 0.0645\n",
      "9 7\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.6259 - acc: 0.0524 - val_loss: 4.5105 - val_acc: 0.0556\n",
      "9 8\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.6103 - acc: 0.0575 - val_loss: 4.4907 - val_acc: 0.0699\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    for j in range(1, 10):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(size + 1, i, input_length=6))\n",
    "        model.add(LSTM(j))\n",
    "        model.add(Dense(size, activation='softmax'))\n",
    "        model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "        model.fit(train_X, train_y, epochs=1, batch_size=1, verbose=2, validation_data=(test_X, test_y));\n",
    "        print (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.6158 - acc: 0.0601 - val_loss: 4.4378 - val_acc: 0.0609\n",
      "11 11\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.5862 - acc: 0.0635 - val_loss: 4.3126 - val_acc: 0.0842\n",
      "11 12\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.6004 - acc: 0.0665 - val_loss: 4.3291 - val_acc: 0.0860\n",
      "11 13\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 52s - loss: 4.5700 - acc: 0.0731 - val_loss: 4.3151 - val_acc: 0.0735\n",
      "11 14\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 52s - loss: 4.6072 - acc: 0.0615 - val_loss: 4.3497 - val_acc: 0.0735\n",
      "11 15\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 52s - loss: 4.5691 - acc: 0.0669 - val_loss: 4.2528 - val_acc: 0.0878\n",
      "11 16\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 1216s - loss: 4.6062 - acc: 0.0635 - val_loss: 4.4208 - val_acc: 0.0824\n",
      "11 17\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 58s - loss: 4.5561 - acc: 0.0715 - val_loss: 4.2968 - val_acc: 0.0806\n",
      "11 18\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 58s - loss: 4.5729 - acc: 0.0669 - val_loss: 4.2220 - val_acc: 0.0896\n",
      "11 19\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.5783 - acc: 0.0589 - val_loss: 4.3688 - val_acc: 0.0591\n",
      "12 11\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.6063 - acc: 0.0613 - val_loss: 4.3724 - val_acc: 0.0753\n",
      "12 12\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.5840 - acc: 0.0633 - val_loss: 4.2904 - val_acc: 0.0896\n",
      "12 13\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 4.6112 - acc: 0.0583 - val_loss: 4.4756 - val_acc: 0.0502\n",
      "12 14\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.6106 - acc: 0.0615 - val_loss: 4.4320 - val_acc: 0.0663\n",
      "12 15\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.5479 - acc: 0.0695 - val_loss: 4.2485 - val_acc: 0.0896\n",
      "12 16\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 4.6110 - acc: 0.0611 - val_loss: 4.4164 - val_acc: 0.0806\n",
      "12 17\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 4.5247 - acc: 0.0802 - val_loss: 4.2231 - val_acc: 0.0860\n",
      "12 18\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 56s - loss: 4.6061 - acc: 0.0635 - val_loss: 4.4149 - val_acc: 0.0789\n",
      "12 19\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 56s - loss: 4.5965 - acc: 0.0550 - val_loss: 4.3807 - val_acc: 0.0753\n",
      "13 11\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 4.6050 - acc: 0.0613 - val_loss: 4.4655 - val_acc: 0.0645\n",
      "13 12\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 4.6027 - acc: 0.0623 - val_loss: 4.3851 - val_acc: 0.0645\n",
      "13 13\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 56s - loss: 4.5901 - acc: 0.0703 - val_loss: 4.3616 - val_acc: 0.0735\n",
      "13 14\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 56s - loss: 4.5974 - acc: 0.0562 - val_loss: 4.4708 - val_acc: 0.0412\n",
      "13 15\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 57s - loss: 4.6015 - acc: 0.0562 - val_loss: 4.3884 - val_acc: 0.0824\n",
      "13 16\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 57s - loss: 4.6110 - acc: 0.0569 - val_loss: 4.4685 - val_acc: 0.0556\n",
      "13 17\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 57s - loss: 4.5580 - acc: 0.0695 - val_loss: 4.2671 - val_acc: 0.0914\n",
      "13 18\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "for i in range(11, 20):\n",
    "    for j in range(11, 20):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(size + 1, i, input_length=6))\n",
    "        model.add(LSTM(j))\n",
    "        model.add(Dense(size, activation='softmax'))\n",
    "        model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "        model.fit(train_X, train_y, epochs=1, batch_size=1, verbose=2, validation_data=(test_X, test_y));\n",
    "        print (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 34s - loss: 4.5627 - acc: 0.0667 - val_loss: 4.2181 - val_acc: 0.0950\n",
      "21 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.5260 - acc: 0.0765 - val_loss: 4.1930 - val_acc: 0.1004\n",
      "21 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.4857 - acc: 0.0783 - val_loss: 4.1343 - val_acc: 0.0968\n",
      "21 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 4.4851 - acc: 0.0812 - val_loss: 4.1343 - val_acc: 0.0914\n",
      "21 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 36s - loss: 4.5822 - acc: 0.0611 - val_loss: 4.3339 - val_acc: 0.0753\n",
      "21 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 36s - loss: 4.4736 - acc: 0.0802 - val_loss: 4.1273 - val_acc: 0.0896\n",
      "21 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 36s - loss: 4.4976 - acc: 0.0751 - val_loss: 4.1283 - val_acc: 0.0932\n",
      "21 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.4882 - acc: 0.0753 - val_loss: 4.1140 - val_acc: 0.0950\n",
      "21 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.4105 - acc: 0.0878 - val_loss: 4.0947 - val_acc: 0.0914\n",
      "21 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.4720 - acc: 0.0870 - val_loss: 4.1271 - val_acc: 0.0932\n",
      "22 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.5704 - acc: 0.0669 - val_loss: 4.2892 - val_acc: 0.0878\n",
      "22 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.4496 - acc: 0.0816 - val_loss: 4.0836 - val_acc: 0.1129\n",
      "22 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 37s - loss: 4.5078 - acc: 0.0693 - val_loss: 4.1453 - val_acc: 0.0842\n",
      "22 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.4769 - acc: 0.0842 - val_loss: 4.0868 - val_acc: 0.1039\n",
      "22 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 38s - loss: 4.5156 - acc: 0.0777 - val_loss: 4.1239 - val_acc: 0.0968\n",
      "22 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.4534 - acc: 0.0840 - val_loss: 4.1264 - val_acc: 0.0950\n",
      "22 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.4494 - acc: 0.0896 - val_loss: 4.0459 - val_acc: 0.0824\n",
      "22 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.4957 - acc: 0.0793 - val_loss: 4.1075 - val_acc: 0.0932\n",
      "22 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.5106 - acc: 0.0751 - val_loss: 4.1317 - val_acc: 0.0842\n",
      "23 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.4230 - acc: 0.0850 - val_loss: 4.1052 - val_acc: 0.0968\n",
      "23 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.4744 - acc: 0.0834 - val_loss: 4.1419 - val_acc: 0.0914\n",
      "23 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 39s - loss: 4.5248 - acc: 0.0739 - val_loss: 4.1583 - val_acc: 0.0878\n",
      "23 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.5212 - acc: 0.0769 - val_loss: 4.1638 - val_acc: 0.1075\n",
      "23 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.5118 - acc: 0.0747 - val_loss: 4.1092 - val_acc: 0.1057\n",
      "23 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.5378 - acc: 0.0761 - val_loss: 4.1675 - val_acc: 0.0878\n",
      "23 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.4287 - acc: 0.0886 - val_loss: 4.0788 - val_acc: 0.1075\n",
      "23 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.4193 - acc: 0.0876 - val_loss: 4.0638 - val_acc: 0.1039\n",
      "23 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 40s - loss: 4.5049 - acc: 0.0789 - val_loss: 4.1611 - val_acc: 0.0896\n",
      "24 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.5176 - acc: 0.0767 - val_loss: 4.2254 - val_acc: 0.1004\n",
      "24 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 4.4936 - acc: 0.0828 - val_loss: 4.1685 - val_acc: 0.0914\n",
      "24 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.5014 - acc: 0.0808 - val_loss: 4.0886 - val_acc: 0.1147\n",
      "24 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 42s - loss: 4.5364 - acc: 0.0671 - val_loss: 4.1768 - val_acc: 0.0932\n",
      "24 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 42s - loss: 4.5156 - acc: 0.0804 - val_loss: 4.1102 - val_acc: 0.0968\n",
      "24 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 42s - loss: 4.4794 - acc: 0.0818 - val_loss: 4.1515 - val_acc: 0.0950\n",
      "24 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.4762 - acc: 0.0785 - val_loss: 4.1051 - val_acc: 0.0986\n",
      "24 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 43s - loss: 4.3731 - acc: 0.0944 - val_loss: 4.0055 - val_acc: 0.1111\n",
      "24 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.4795 - acc: 0.0787 - val_loss: 4.1222 - val_acc: 0.0932\n",
      "25 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.4707 - acc: 0.0757 - val_loss: 4.1657 - val_acc: 0.0968\n",
      "25 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.4974 - acc: 0.0763 - val_loss: 4.1442 - val_acc: 0.0950\n",
      "25 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.5118 - acc: 0.0791 - val_loss: 4.1928 - val_acc: 0.0914\n",
      "25 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 44s - loss: 4.4425 - acc: 0.0802 - val_loss: 4.0894 - val_acc: 0.0824\n",
      "25 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.4298 - acc: 0.0836 - val_loss: 4.0739 - val_acc: 0.0896\n",
      "25 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.5692 - acc: 0.0713 - val_loss: 4.2063 - val_acc: 0.1147\n",
      "25 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.4332 - acc: 0.0914 - val_loss: 4.1177 - val_acc: 0.1057\n",
      "25 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.4010 - acc: 0.0886 - val_loss: 4.0666 - val_acc: 0.1039\n",
      "25 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.5480 - acc: 0.0733 - val_loss: 4.2210 - val_acc: 0.0914\n",
      "26 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 45s - loss: 4.4888 - acc: 0.0789 - val_loss: 4.1355 - val_acc: 0.1022\n",
      "26 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.4594 - acc: 0.0785 - val_loss: 4.1217 - val_acc: 0.0914\n",
      "26 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.4176 - acc: 0.0834 - val_loss: 4.0916 - val_acc: 0.1022\n",
      "26 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.4359 - acc: 0.0769 - val_loss: 4.0947 - val_acc: 0.0968\n",
      "26 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 46s - loss: 4.5371 - acc: 0.0761 - val_loss: 4.1501 - val_acc: 0.0986\n",
      "26 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.4646 - acc: 0.0828 - val_loss: 4.1001 - val_acc: 0.1111\n",
      "26 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 47s - loss: 4.4524 - acc: 0.0812 - val_loss: 4.0743 - val_acc: 0.0968\n",
      "26 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 48s - loss: 4.4387 - acc: 0.0908 - val_loss: 4.0512 - val_acc: 0.1165\n",
      "26 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 48s - loss: 4.5023 - acc: 0.0862 - val_loss: 4.2332 - val_acc: 0.0932\n",
      "27 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 48s - loss: 4.3998 - acc: 0.0924 - val_loss: 4.1189 - val_acc: 0.0986\n",
      "27 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.5279 - acc: 0.0761 - val_loss: 4.1134 - val_acc: 0.1093\n",
      "27 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 48s - loss: 4.5174 - acc: 0.0747 - val_loss: 4.1635 - val_acc: 0.0968\n",
      "27 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 48s - loss: 4.4518 - acc: 0.0844 - val_loss: 4.0207 - val_acc: 0.1129\n",
      "27 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.4852 - acc: 0.0793 - val_loss: 4.0736 - val_acc: 0.1147\n",
      "27 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 49s - loss: 4.4580 - acc: 0.0830 - val_loss: 4.0606 - val_acc: 0.1057\n",
      "27 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.4850 - acc: 0.0804 - val_loss: 4.0874 - val_acc: 0.0986\n",
      "27 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.4075 - acc: 0.0892 - val_loss: 4.0459 - val_acc: 0.1147\n",
      "27 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 4.4495 - acc: 0.0900 - val_loss: 4.0954 - val_acc: 0.0986\n",
      "28 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 50s - loss: 4.4357 - acc: 0.0838 - val_loss: 4.1051 - val_acc: 0.0717\n",
      "28 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.4869 - acc: 0.0795 - val_loss: 4.1515 - val_acc: 0.0789\n",
      "28 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.4184 - acc: 0.0866 - val_loss: 4.0752 - val_acc: 0.1075\n",
      "28 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.4719 - acc: 0.0783 - val_loss: 4.0973 - val_acc: 0.1022\n",
      "28 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.4710 - acc: 0.0834 - val_loss: 4.0953 - val_acc: 0.0950\n",
      "28 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.4219 - acc: 0.0862 - val_loss: 4.0499 - val_acc: 0.1057\n",
      "28 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 52s - loss: 4.4462 - acc: 0.0846 - val_loss: 4.0670 - val_acc: 0.0896\n",
      "28 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 51s - loss: 4.4471 - acc: 0.0870 - val_loss: 4.0643 - val_acc: 0.0950\n",
      "28 29\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 52s - loss: 4.5051 - acc: 0.0872 - val_loss: 4.1191 - val_acc: 0.1093\n",
      "29 21\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 184s - loss: 4.5309 - acc: 0.0703 - val_loss: 4.1672 - val_acc: 0.0771\n",
      "29 22\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 53s - loss: 4.4972 - acc: 0.0791 - val_loss: 4.1130 - val_acc: 0.0878\n",
      "29 23\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 53s - loss: 4.4594 - acc: 0.0824 - val_loss: 4.0788 - val_acc: 0.1093\n",
      "29 24\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.4739 - acc: 0.0783 - val_loss: 4.1222 - val_acc: 0.1022\n",
      "29 25\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.4211 - acc: 0.0888 - val_loss: 4.0841 - val_acc: 0.0878\n",
      "29 26\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.4239 - acc: 0.0856 - val_loss: 4.0749 - val_acc: 0.1004\n",
      "29 27\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 54s - loss: 4.4364 - acc: 0.0888 - val_loss: 4.0602 - val_acc: 0.0932\n",
      "29 28\n",
      "Train on 5022 samples, validate on 558 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 4.4217 - acc: 0.0844 - val_loss: 4.0566 - val_acc: 0.0986\n",
      "29 29\n"
     ]
    }
   ],
   "source": [
    "for i in range(21, 30):\n",
    "    for j in range(21, 30):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(size + 1, i, input_length=6))\n",
    "        model.add(LSTM(j))\n",
    "        model.add(Dense(size, activation='softmax'))\n",
    "        model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "        model.fit(train_X, train_y, epochs=1, batch_size=1, verbose=2, validation_data=(test_X, test_y));\n",
    "        print (i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем параметры 26 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.set_yscale('log')\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
    "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot = PlotLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f80f8980eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(size + 1, 26, input_length=6))\n",
    "model.add(LSTM(29))\n",
    "model.add(Dense(size, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=39, batch_size=1, verbose=2, validation_data=(test_X, test_y), callbacks=[plot]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_chords = np.array(test_X[:1])\n",
    "answer = test_X[0]\n",
    "\n",
    "for i in range(0, len(test_X)-1):\n",
    "    current_chord = (model.predict(predicted_chords[i:i+1])).argmax()\n",
    "    answer = np.append(answer, current_chord)\n",
    "    new_chords = predicted_chords[i][-5:]\n",
    "    new_chords = np.append(new_chords, current_chord)\n",
    "    predicted_chords = np.vstack((predicted_chords, new_chords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_answer = []\n",
    "\n",
    "for chord in answer:\n",
    "    encoded_answer.append(unique_chords[chord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([24., 29., 32.,  0.,  0.]), array([22., 26., 32.,  0.,  0.]), array([22., 26.,  0.,  0.,  0.]), array([22., 27., 31.,  0.,  0.]), array([22., 25., 28., 31.,  0.]), array([24., 27., 29., 33.,  0.]), array([22., 25., 29.,  0.,  0.]), array([22., 24., 25., 29.,  0.]), array([22., 25., 29.,  0.,  0.]), array([24., 29., 33.,  0.,  0.]), array([24., 27., 29., 33.,  0.]), array([22., 24., 29.,  0.,  0.]), array([24., 29., 33.,  0.,  0.]), array([24., 27., 31.,  0.,  0.]), array([22., 24., 27., 31.,  0.]), array([24., 26., 29., 31.,  0.]), array([24., 27., 29., 33.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 27., 31.,  0.,  0.]), array([22., 27., 31., 32.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29., 32.,  0.]), array([22., 27., 31.,  0.,  0.]), array([22., 27., 31.,  0.,  0.]), array([24., 27., 31.,  0.,  0.]), array([24., 27., 31., 33.,  0.]), array([22., 26., 29.,  0.,  0.]), array([24., 27., 33.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 27., 31.,  0.]), array([24., 29., 33.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([24., 29., 33.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29., 33.,  0.]), array([22., 24., 28., 31.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([24., 29., 33.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 27., 31.,  0.,  0.]), array([26., 29., 32.,  0.,  0.]), array([22., 27., 31.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 27., 31.,  0.,  0.]), array([24., 27., 32.,  0.,  0.]), array([22., 26., 29.,  0.,  0.]), array([22., 26., 29., 32.,  0.]), array([22., 27., 31.,  0.,  0.]), array([22., 27., 31., 32.,  0.]), array([24., 27., 32.,  0.,  0.]), array([24., 27., 29., 32.,  0.]), array([22., 26., 29., 32.,  0.]), array([22., 27., 31.,  0.,  0.]), array([22., 25., 27., 31.,  0.]), array([24., 27., 32.,  0.,  0.]), array([25., 29., 32.,  0.,  0.]), array([24., 27., 31., 32.,  0.]), array([22., 25., 29.,  0.,  0.]), array([22., 25., 29., 31.,  0.]), array([24., 27., 32.,  0.,  0.]), array([22., 25., 29.,  0.,  0.]), array([22., 25., 29.,  0.,  0.]), array([22., 25., 31.,  0.,  0.]), array([22., 25., 29., 31.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 29., 31., 32.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([22., 24., 28., 31.,  0.]), array([24., 29., 32.,  0.,  0.]), array([24., 28., 31.,  0.,  0.]), array([24., 29., 32.,  0.,  0.])]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords = []\n",
    "\n",
    "for encoded_chord in encoded_answer:\n",
    "    notes = []\n",
    "    for encoded_note in encoded_chord:\n",
    "        if encoded_note != 0.:\n",
    "            note = structures.Note(int(encoded_note) + 50)\n",
    "            notes.append(note)\n",
    "    chord = structures.Chord(notes, 32, 127)\n",
    "    chords.append(chord)\n",
    "    \n",
    "track = [structures.Track(chords=chords)]\n",
    "song = structures.Song(tracks=track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song.save(\"bach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
